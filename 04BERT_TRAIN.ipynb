{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Sentence Pair Classification with BERT\n",
    "### Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from bert import data, model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "# change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "ctx = mx.gpu(0)\n",
    "# ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the pre-trained BERT model\n",
    "\n",
    "The list of pre-trained BERT models available\n",
    "in GluonNLP can be found\n",
    "[here](../../model_zoo/bert/index.rst).\n",
    "\n",
    "In this\n",
    "tutorial, the BERT model we will use is BERT\n",
    "BASE trained on an uncased corpus of books and\n",
    "the English Wikipedia dataset in the\n",
    "GluonNLP model zoo.\n",
    "\n",
    "### Get BERT\n",
    "\n",
    "Let's first take\n",
    "a look at the BERT model\n",
    "architecture for sentence pair classification below:\n",
    "<div style=\"width:\n",
    "500px;\">![bert-sentence-pair](bert-sentence-pair.png)</div>\n",
    "where the model takes a pair of\n",
    "sequences and pools the representation of the\n",
    "first token in the sequence.\n",
    "Note that the original BERT model was trained for a\n",
    "masked language model and next-sentence prediction tasks, which includes layers\n",
    "for language model decoding and\n",
    "classification. These layers will not be used\n",
    "for fine-tuning the sentence pair classification.\n",
    "\n",
    "We can load the\n",
    "pre-trained BERT fairly easily\n",
    "using the model API in GluonNLP, which returns the vocabulary\n",
    "along with the\n",
    "model. We include the pooler layer of the pre-trained model by setting\n",
    "`use_pooler` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=28996, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n"
     ]
    }
   ],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',#book_corpus_wiki_en_uncased\n",
    "                                             dataset_name='biobert_v1.1_pubmed_cased', #biobert_v1.1_pubmed_cased\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "# print(bert_base)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the model for `SentencePair` classification\n",
    "\n",
    "Now that we have loaded\n",
    "the BERT model, we only need to attach an additional layer for classification.\n",
    "The `BERTClassifier` class uses a BERT base model to encode sentence\n",
    "representation, followed by a `nn.Dense` layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier = model.classification.BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "bert_classifier.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "bert_classifier.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = mx.gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for BERT\n",
    "\n",
    "For this tutorial, we need to do a bit of preprocessing before feeding our data introduced\n",
    "the BERT model. Here we want to leverage the dataset included in the downloaded archive at the\n",
    "beginning of this tutorial.\n",
    "\n",
    "### Loading the dataset\n",
    "\n",
    "We use\n",
    "the dev set of the\n",
    "Microsoft Research Paraphrase Corpus dataset. The file is\n",
    "named 'dev.tsv'. Let's take a look at the first few lines of the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moreover, Wnt-1-inducible secreted protein-1 (WISP-1), which is a responsive gene of Wnt activation , can promote angiogenesis in post-MI heart via regulating histone deacetylase [[**##**]].\n",
      "Angiokine Wisp-1 is increased in myocardial infarction and regulates cardiac endothelial signaling. Myocardial infarctions (MIs) cause the loss of myocytes due to lack of sufficient oxygenation and latent revascularization. Although the administration of histone deacetylase (HDAC) inhibitors reduces the size of infarctions and improves cardiac physiology in small-animal models of MI injury, the cellular targets of the HDACs, which the drugs inhibit, are largely unspecified. Here, we show that WNT-inducible secreted protein-1 (Wisp-1), a matricellular protein that promotes angiogenesis in cancers as well as cell survival in isolated cardiac myocytes and neurons, is a target of HDACs. Further, Wisp-1 transcription is regulated by HDACs and can be modified by the HDAC inhibitor, suberanilohydroxamic acid (SAHA/vorinostat), after MI injury. We observe that, at 7 days after MI, Wisp-1 is elevated 3-fold greater in the border zone of infarction in mice that experience an MI injury and are injected daily with SAHA, relative to MI alone. Additionally, human coronary artery endothelial cells (HCAECs) produce WISP-1 and are responsive to autocrine WISP-1-mediated signaling, which functionally promotes their proangiogenic behavior. Altering endogenous expression of WISP-1 in HCAECs directly impacts their network density in vitro. Therapeutic interventions after a heart attack define the extent of infarct injury, cell survival, and overall prognosis. Our studies shown here identify a potentially novel cardiac angiokine, Wisp-1, that may contribute to beneficial post-MI treatment modalities.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Skip the first line, which is the schema\n",
    "num_discard_samples = 1\n",
    "# Split fields by tabs\n",
    "field_separator = nlp.data.Splitter('\\t')\n",
    "# Fields to select from the file\n",
    "field_indices = [3, 4, 0]\n",
    "data_train_raw = nlp.data.TSVDataset(filename='bert_train.tsv',\n",
    "                                 field_separator=field_separator,\n",
    "                                 num_discard_samples=num_discard_samples,\n",
    "                                 field_indices=field_indices)\n",
    "# data_train_val_raw = nlp.data.TSVDataset(filename='bert_train_val.tsv',\n",
    "#                                  field_separator=field_separator,\n",
    "#                                  num_discard_samples=num_discard_samples,\n",
    "#                                  field_indices=field_indices)\n",
    "\n",
    "\n",
    "sample_id = 0\n",
    "# Sentence A\n",
    "print(data_train_raw[sample_id][0])\n",
    "# Sentence B\n",
    "print(data_train_raw[sample_id][1])\n",
    "# 1 means equivalent, 0 means not equivalent\n",
    "print(data_train_raw[sample_id][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the pre-trained BERT model, we need to pre-process the data in the same\n",
    "way it was trained. The following figure shows the input representation in BERT:\n",
    "<div style=\"width: 500px;\">![bert-embed](bert-embed.png)</div>\n",
    "\n",
    "We will use\n",
    "`BERTDatasetTransform` to perform the following transformations:\n",
    "- tokenize\n",
    "the\n",
    "input sequences\n",
    "- insert [CLS] at the beginning\n",
    "- insert [SEP] between sentence\n",
    "A and sentence B, and at the end\n",
    "- generate segment ids to indicate whether\n",
    "a token belongs to the first sequence or the second sequence.\n",
    "- generate valid length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary used for tokenization = \n",
      "Vocab(size=28996, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
      "[PAD] token id = 0\n",
      "[CLS] token id = 101\n",
      "[SEP] token id = 102\n",
      "token ids = \n",
      "[  101  9841   117   160  2227   118   122   118  1107  7641 16240  3318\n",
      "  1174  4592   118   122   113   160  6258  2101   118   122   114   117\n",
      "  1134  1110   170  1231 20080  4199  2109  5565  1104   160  2227 14915\n",
      "   117  1169  4609  1126 10712 27364  1107  2112   118 26574  1762  2258\n",
      " 24717  1117  4793  1260  7954  2340 26572   164   164   115   115   108\n",
      "   108   115   115   166   166   119   102 26285  2660  4314  1162   160\n",
      "  1548  1643   118   122  1110  2569  1107  1139 13335  2881  2916  1107\n",
      " 14794  5796  1105 16146  1116 17688  1322 12858 21091  1348 16085   119\n",
      "  1422 13335  2881  2916  1107 14794 13945   113 26574  1116   114  2612\n",
      "  1103  2445  1104  1139 26431  1496  1106  2960  1104  6664  7621  1891\n",
      "  1105  1523  2227  1231 11509 11702  2734   119  1966  1103  3469  1104\n",
      "  1117  4793  1260  7954  2340 26572   113 10728  8101   114 27558  1116\n",
      " 13822  1103  2060  1104  1107 14794 13945  1105  4607  1116 17688 25445\n",
      "  1107  1353   118  3724  3584  1104 26574  3773   117  1103 14391  7539\n",
      "  1104  1103 10728  8101  1116   117  1134  1103  5557  1107 23034   117\n",
      "  1132  3494  8362 20080 10294  6202   119  3446   117  1195  1437  1115\n",
      "   160 15681   118  1107  7641 16240  3318  1174  4592   118   122   113\n",
      "   160  1548  1643   118   122   114   117   170 22591 10835  2339  5552\n",
      "  4592  1115 14710  1126 10712 27364  1107  4182  1116  1112  1218  1112\n",
      "  2765  8115  1107  6841 17688  1139 26431  1105 16993   117  1110   170\n",
      "  4010  1104 10728  8101  1116   119  6940   117   160  1548  1643   118\n",
      "   122 15416  1110 12521  1118 10728  8101  1116  1105  1169  1129  5847\n",
      "  1118  1103 10728  8101 27558   117  4841 19776 24755  7889 23632 10649\n",
      " 11787  1665  5190   113 13411 11612   120   191  9012 14226 19756   114\n",
      "   117  1170 26574  3773   119  1284 12326  1115   117  1120   128  1552\n",
      "  1170 26574   117   160  1548  1643   118   122  1110  8208   124   118\n",
      " 11203  3407  1107  1103  3070  4834  1104  1107 14794  5796  1107 14105\n",
      "  1115  2541  1126 26574  3773  1105  1132 21881  3828  1114 13411 11612\n",
      "   117  5236  1106 26574  2041   119  5533   117  1769  1884 15789  1616\n",
      " 18593  1322 12858 21091  1348  3652   113 18315  1592  8231  1116   114\n",
      "  3133   160  6258  2101   118   122  1105  1132  1231 20080  4199  2109\n",
      "  1106 12365  1665  8643   160  6258  2101   118   122   118 22060 16085\n",
      "   117  1134  8458  1193 14710  1147  5250  4993  2660 19438  4658   119\n",
      " 14983  5938  1322 19790  2285  2838  1104   160  6258  2101   118   122\n",
      "  1107 18315  1592  8231  1116  2626 15791  1147  2443  3476  1107   191\n",
      "  2875  2180   119  1109 14543 14272  2941 22496  1170   170  1762  2035\n",
      "  9410  1103  6102  1104  1107 14794  5822  3773   117  2765  8115   117\n",
      "  1105  2905  5250 25566  4863   119  3458  2527  2602  1303  6183   170\n",
      "  9046  2281 17688  1126 10712  4314  1162   117   160  1548  1643   118\n",
      "   122   117  1115  1336  8681  1106 16250  2112   118 26574  3252   182\n",
      " 16848 16652   119   102     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "valid length = \n",
      "484\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=False)\n",
    "max_len = 512\n",
    "all_labels = [\"0\", \"1\"]\n",
    "\n",
    "# whether to transform the data as sentence pairs.\n",
    "# for single sentence classification, set pair=False\n",
    "# for regression task, set class_labels=None\n",
    "# for inference without label available, set has_label=False\n",
    "pair = True\n",
    "transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                                class_labels=all_labels,\n",
    "                                                has_label=True,\n",
    "                                                pad=True,\n",
    "                                                pair=pair)\n",
    "\n",
    "data_train = data_train_raw.transform(transform)\n",
    "# data_train_val = data_train_val_raw.transform(transform)\n",
    "\n",
    "print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))\n",
    "print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))\n",
    "print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))\n",
    "print('token ids = \\n%s'%data_train[sample_id][0])\n",
    "print('valid length = \\n%s'%data_train[sample_id][1])\n",
    "print('segment ids = \\n%s'%data_train[sample_id][2])\n",
    "print('label = \\n%s'%data_train[sample_id][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249934"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "Now we have all the pieces to put together, and we can finally start fine-tuning the\n",
    "model with very few epochs. For demonstration, we use a fixed learning rate and\n",
    "skip the validation steps. For the optimizer, we leverage the ADAM optimizer which\n",
    "performs very well for NLP data and for BERT models in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_sampler = nlp.data.FixedBucketSampler(lengths=[123] * len(data_train),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler, prefetch=500, num_workers=2)\n",
    "\n",
    "# train_val_sampler = nlp.data.FixedBucketSampler(lengths=[123] * len(data_train_val),\n",
    "#                                             batch_size=batch_size * 2,\n",
    "#                                             shuffle=False)\n",
    "# bert_dataloader_val = mx.gluon.data.DataLoader(data_train_val, batch_sampler=train_val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-6\n",
    "trainer = mx.gluon.Trainer(bert_classifier.collect_params(), 'adam',\n",
    "                           {'learning_rate': lr, 'epsilon': 1e-9})\n",
    "\n",
    "# Collect all differentiable parameters\n",
    "# `grad_req == 'null'` indicates no gradients are calculated (e.g. constant parameters)\n",
    "# The gradients for these params are clipped later\n",
    "params = [p for p in bert_classifier.collect_params().values() if p.grad_req != 'null']\n",
    "grad_clip = 1\n",
    "\n",
    "# Training the model with only three epochs\n",
    "log_interval = 512\n",
    "test_log_interval = 32 * 10\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(dataset):\n",
    "#     total_L = 0.0\n",
    "#     total_sample_num = 0\n",
    "#     total_correct_num = 0\n",
    "#     start_log_interval_time = time.time()\n",
    "    \n",
    "#     t_metric = mx.metric.Accuracy()\n",
    "\n",
    "#     print('Begin Testing...')\n",
    "#     for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(dataset):\n",
    "        \n",
    "#         # Load the data to the GPU\n",
    "#         token_ids = token_ids.as_in_context(ctx)\n",
    "#         valid_length = valid_length.as_in_context(ctx)\n",
    "#         segment_ids = segment_ids.as_in_context(ctx)\n",
    "        \n",
    "#         label = label.as_in_context(ctx)\n",
    "#         out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "\n",
    "        \n",
    "#         t_metric.update([label], [out])\n",
    "\n",
    "#         if (batch_id + 1) % test_log_interval == 0:\n",
    "#             print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "#                 batch_id + 1, len(dataset),\n",
    "#                 time.time() - start_log_interval_time))\n",
    "#             start_log_interval_time = time.time()\n",
    "#     return t_metric.get()[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 1/312484] loss=0.0014, lr=0.0000050, acc=0.500, conf=0.0 of 2\n",
      "[-0.01505742 -0.10734081 -0.14366901 -0.15694007]\n",
      "[Epoch 0 Batch 513/312484] loss=0.4042, lr=0.0000050, acc=0.848, conf=1.0 of 4\n",
      "[1.2760909 1.4083455 1.505771  1.4063264]\n",
      "[Epoch 0 Batch 1025/312484] loss=0.4152, lr=0.0000050, acc=0.859, conf=2.0 of 2\n",
      "[ 1.5577323 -1.9759296 -1.7627802  1.8772725]\n",
      "[Epoch 0 Batch 1537/312484] loss=0.4015, lr=0.0000050, acc=0.869, conf=0.0 of 0\n",
      "[-2.454082  -2.3445866 -2.1327221 -1.2225958]\n",
      "[Epoch 0 Batch 2049/312484] loss=0.4211, lr=0.0000050, acc=0.874, conf=0.0 of 2\n",
      "[ 0.6504191  -2.4373806   0.10464716 -2.4724102 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-de92538836e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Gradient clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallreduce_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_global_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\yingchengxuan\\Miniconda3\\lib\\site-packages\\gluonnlp\\utils\\parameter.py\u001b[0m in \u001b[0;36mclip_grad_global_norm\u001b[1;34m(parameters, max_norm, check_isfinite)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_isfinite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             warnings.warn(\n",
      "\u001b[1;32mD:\\yingchengxuan\\Miniconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2565\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2566\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2567\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2569\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\yingchengxuan\\Miniconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2547\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2548\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2549\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   2550\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(bert_dataloader):\n",
    "        with mx.autograd.record():\n",
    "\n",
    "            # Load the data to the GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # Forward computation\n",
    "            out = bert_classifier(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # And backwards computation\n",
    "        ls.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "        trainer.update(1)\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Printing vital information\n",
    "        if (batch_id) % (log_interval) == 0:\n",
    "            conf_rate = '{} {} {}'.format((out[:,1]>1.5).sum().asnumpy()[0], 'of',(label==1).sum().asnumpy()[0])\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}, conf={}'\n",
    "                         .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1],\n",
    "                                 conf_rate))\n",
    "            print(out[:,1].asnumpy())\n",
    "            step_loss = 0\n",
    "            \n",
    "        if (batch_id + 1) % (20000) == 0:\n",
    "            bert_classifier.save_parameters('bisai/epoch{}.params'.format(batch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d0860ec2e342f4b8124114af435f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Dask Apply', max=256, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7607497c234c96afab880cca319755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Dask Apply', max=256, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from bert import data, model\n",
    "import time\n",
    "from mxnet.gluon.utils import split_and_load \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "\n",
    "val = pd.read_hdf('cleaned.h5', 'val')\n",
    "train = pd.read_hdf('cleaned.h5', 'train')\n",
    "candid = pd.read_hdf('cleaned.h5', 'candid')\n",
    "candid = candid.reset_index(drop=True)\n",
    "\n",
    "train = train.replace('\\\\n','', regex=True)\n",
    "train = train.replace('\\\\r','', regex=True)\n",
    "candid = candid.replace('\\\\n','', regex=True)\n",
    "candid = candid.replace('\\\\r','', regex=True)\n",
    "val = val.replace('\\\\n','', regex=True)\n",
    "val = val.replace('\\\\r','', regex=True)\n",
    "\n",
    "candid.abstract = candid.abstract.swifter.allow_dask_on_strings().apply(lambda x: ''.join([\" \" if ord(i) < 32 or ord(i) > 126 else i for i in x]))\n",
    "candid.title = candid.title.swifter.allow_dask_on_strings().apply(lambda x: ''.join([\" \" if ord(i) < 32 or ord(i) > 126 else i for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25_rank_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_rank_val = np.load('test_recall.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25_rank_train = np.load('train_recall.npy')\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "# change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "# ctx = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3)]\n",
    "ctx = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3), mx.gpu(4), mx.gpu(5), mx.gpu(6), mx.gpu(7)]\n",
    "# ctx = mx.cpu()\n",
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',#book_corpus_wiki_en_uncased\n",
    "                                             dataset_name='biobert_v1.1_pubmed_cased', #biobert_v1.1_pubmed_cased\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "bert_classifier = model.classification.BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "bert_classifier.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "bert_classifier.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = mx.gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()\n",
    "# bert_classifier.load_parameters('bisai/epoch121599.params')\n",
    "bert_classifier.load_parameters('bisai/512new/epoch121599.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25_rank_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_set(start=0,end=20,mask=None):\n",
    "    idx2pid = candid['paper_id'].to_dict()\n",
    "    pid2idx = {v: k for k, v in idx2pid.items()}\n",
    "    test_set = []\n",
    "    \n",
    "    des = val.description_text.to_list()\n",
    "    tit = candid.title.to_list()\n",
    "    abst = candid.abstract.to_list()\n",
    "    \n",
    "    for idx,row in enumerate(bm25_rank_val):\n",
    "        row = row[start:end]\n",
    "            \n",
    "        if mask is not None:\n",
    "            if mask[idx]:\n",
    "                continue\n",
    "                \n",
    "        for pid in row:\n",
    "            t = {\n",
    "                'relavence': 'UNKOWN',\n",
    "                'qidx': idx,\n",
    "                'pidx': pid,\n",
    "                'q_text': des[idx],\n",
    "                'd_text': tit[pid] + ' ' + abst[pid]\n",
    "            }\n",
    "            \n",
    "            test_set.append(t)\n",
    "            \n",
    "        if (idx % 10000 == 0):\n",
    "            print(idx)\n",
    "    print(len(bm25_rank_val))\n",
    "    pd.DataFrame(test_set).to_csv('val_bert_50.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score():\n",
    "    num_discard_samples = 1\n",
    "    field_separator = nlp.data.Splitter('\\t')\n",
    "    field_indices = [3, 4, 0]\n",
    "    data_train_raw = nlp.data.TSVDataset(filename='val_bert_50.tsv', \n",
    "                                         # change val_bert or test_bert_train_val val_bert_50\n",
    "                                     field_separator=field_separator,\n",
    "                                     num_discard_samples=num_discard_samples,\n",
    "                                     field_indices=field_indices)\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=False)\n",
    "    max_len = 512\n",
    "    all_labels = [\"0\", \"UNKOWN\"]\n",
    "\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    # for regression task, set class_labels=None\n",
    "    # for inference without label available, set has_label=False\n",
    "    pair = True\n",
    "    transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                                    class_labels=all_labels,\n",
    "                                                    has_label=True,\n",
    "                                                    pad=True,\n",
    "                                                    pair=pair)\n",
    "\n",
    "\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "    # Training the model with only three epochs\n",
    "\n",
    "    batch_size = 500\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_size=batch_size, num_workers=40,\n",
    "                                               pin_memory=True, prefetch=20000)\n",
    "\n",
    "    log_interval = 160\n",
    "    test_log_interval = 320\n",
    "\n",
    "    start_log_interval_time = time.time()\n",
    "    new_recall_score = []\n",
    "    \n",
    "    cntdy0 = 0\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(bert_dataloader):\n",
    "        token_ids = split_and_load(token_ids, ctx, even_split=False)\n",
    "        valid_length = split_and_load(valid_length.astype('float32'), ctx, even_split=False)\n",
    "        segment_ids = split_and_load(segment_ids, ctx, even_split=False)\n",
    "        label = [bert_classifier(a, b, c)\n",
    "                      for a, b, c in zip(token_ids, segment_ids, valid_length)]\n",
    "\n",
    "        label = [l[:,1].asnumpy() for l in label]\n",
    "        new_recall_score.extend(label)\n",
    "        \n",
    "        for i in range(8):\n",
    "            cntdy0 += sum(label[i]>0)\n",
    "            \n",
    "        if batch_id % 50 == 0:\n",
    "            print(cntdy0)\n",
    "            \n",
    "        if (batch_id + 1) % test_log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                batch_id + 1, len(bert_dataloader),\n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    return new_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_rows(mask,scores,k=10):\n",
    "    init_score = np.ones([len(mask),k]) * -1000\n",
    "    cnt = 0\n",
    "    for idx,row in enumerate(mask):\n",
    "        if row==False:\n",
    "            init_score[idx,:]=scores[cnt,:]\n",
    "            cnt+=1\n",
    "    return init_score\n",
    "# scatter_rows(np.array([True,False,True,False]),\n",
    "#             np.array([[1,2,3,4,5,6,7,8,9,0],[1,2,3,4,5,6,7,8,9,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "59\n",
      "129\n",
      "156\n",
      "227\n",
      "301\n",
      "360\n",
      "[Batch 320/3443] elapsed 1013.20 s\n",
      "389\n",
      "450\n",
      "488\n",
      "553\n",
      "603\n",
      "666\n",
      "[Batch 640/3443] elapsed 995.01 s\n",
      "732\n",
      "806\n",
      "844\n",
      "891\n",
      "964\n",
      "1024\n",
      "1067\n",
      "[Batch 960/3443] elapsed 992.84 s\n",
      "1123\n",
      "1181\n",
      "1221\n",
      "1279\n",
      "1338\n",
      "1421\n",
      "[Batch 1280/3443] elapsed 991.76 s\n",
      "1473\n",
      "1539\n",
      "1605\n",
      "1653\n",
      "1708\n",
      "1764\n",
      "[Batch 1600/3443] elapsed 995.58 s\n",
      "1828\n",
      "1884\n",
      "1926\n",
      "1992\n",
      "2039\n",
      "2091\n",
      "2136\n",
      "[Batch 1920/3443] elapsed 994.78 s\n",
      "2184\n",
      "2235\n",
      "2297\n",
      "2360\n",
      "2416\n",
      "2475\n",
      "[Batch 2240/3443] elapsed 993.73 s\n",
      "2530\n",
      "2573\n",
      "2623\n",
      "2674\n",
      "2742\n",
      "2806\n",
      "2870\n",
      "[Batch 2560/3443] elapsed 995.79 s\n",
      "2922\n",
      "3003\n",
      "3086\n",
      "3130\n",
      "3196\n",
      "3239\n",
      "[Batch 2880/3443] elapsed 996.33 s\n",
      "3317\n",
      "3364\n",
      "3471\n",
      "3550\n",
      "3608\n",
      "3677\n",
      "[Batch 3200/3443] elapsed 992.99 s\n",
      "3747\n",
      "3811\n",
      "3854\n",
      "3934\n",
      "3999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-359:\n",
      "Process ForkPoolWorker-356:\n",
      "Process ForkPoolWorker-340:\n",
      "Process ForkPoolWorker-342:\n",
      "Process ForkPoolWorker-321:\n",
      "Process ForkPoolWorker-319:\n",
      "Process ForkPoolWorker-361:\n",
      "Process ForkPoolWorker-338:\n",
      "Process ForkPoolWorker-318:\n",
      "Process ForkPoolWorker-351:\n",
      "Process ForkPoolWorker-346:\n",
      "Process ForkPoolWorker-326:\n",
      "Process ForkPoolWorker-329:\n",
      "Process ForkPoolWorker-314:\n",
      "Process ForkPoolWorker-316:\n",
      "Process ForkPoolWorker-349:\n",
      "Process ForkPoolWorker-333:\n",
      "Process ForkPoolWorker-358:\n",
      "Process ForkPoolWorker-336:\n",
      "Process ForkPoolWorker-348:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-365:\n"
     ]
    }
   ],
   "source": [
    "k = 50\n",
    "# cal first score\n",
    "gen_test_set(start=0,end=k)\n",
    "np_recall_score = np.hstack(get_score()).reshape([-1, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19875 34428\n"
     ]
    }
   ],
   "source": [
    "treshold = 10\n",
    "mask = np.sum(np_recall_score>treshold, axis=1)>0\n",
    "print(sum(mask), len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start= 450 end= 500\n",
      "0\n",
      "10000\n",
      "20000\n",
      "34428\n",
      "0\n",
      "10\n",
      "18\n",
      "25\n",
      "33\n",
      "49\n",
      "60\n",
      "[Batch 320/1456] elapsed 992.50 s\n",
      "68\n",
      "81\n",
      "92\n",
      "101\n",
      "109\n",
      "119\n",
      "[Batch 640/1456] elapsed 990.68 s\n",
      "130\n",
      "141\n",
      "155\n",
      "165\n",
      "173\n",
      "182\n",
      "193\n",
      "[Batch 960/1456] elapsed 992.01 s\n",
      "198\n",
      "208\n",
      "220\n",
      "235\n",
      "241\n",
      "258\n",
      "[Batch 1280/1456] elapsed 994.95 s\n",
      "272\n",
      "279\n",
      "292\n",
      "303\n",
      "19875 34428\n",
      "(34428, 500)\n",
      "\n",
      "\n",
      "start= 500 end= 550\n",
      "0\n",
      "10000\n",
      "20000\n",
      "34428\n",
      "0\n",
      "10\n",
      "22\n",
      "31\n",
      "36\n",
      "54\n",
      "60\n",
      "[Batch 320/1456] elapsed 997.57 s\n",
      "71\n",
      "77\n",
      "86\n",
      "94\n",
      "105\n",
      "115\n",
      "[Batch 640/1456] elapsed 991.04 s\n",
      "131\n",
      "147\n",
      "154\n",
      "166\n",
      "171\n",
      "180\n",
      "196\n",
      "[Batch 960/1456] elapsed 991.64 s\n",
      "209\n",
      "222\n",
      "229\n",
      "239\n",
      "249\n",
      "256\n",
      "[Batch 1280/1456] elapsed 993.82 s\n",
      "263\n",
      "272\n",
      "278\n",
      "286\n",
      "19875 34428\n",
      "(34428, 550)\n",
      "\n",
      "\n",
      "start= 550 end= 600\n",
      "0\n",
      "10000\n",
      "20000\n",
      "34428\n",
      "0\n",
      "10\n",
      "23\n",
      "30\n",
      "36\n",
      "46\n",
      "57\n",
      "[Batch 320/1456] elapsed 995.25 s\n",
      "68\n",
      "73\n",
      "85\n",
      "96\n",
      "107\n",
      "115\n",
      "[Batch 640/1456] elapsed 993.16 s\n",
      "121\n",
      "130\n",
      "137\n",
      "146\n",
      "154\n",
      "159\n",
      "167\n",
      "[Batch 960/1456] elapsed 995.46 s\n",
      "174\n",
      "180\n",
      "189\n",
      "197\n",
      "212\n",
      "223\n",
      "[Batch 1280/1456] elapsed 993.48 s\n",
      "232\n",
      "236\n",
      "240\n",
      "244\n",
      "19875 34428\n",
      "(34428, 600)\n",
      "\n",
      "\n",
      "start= 600 end= 650\n",
      "0\n",
      "10000\n",
      "20000\n",
      "34428\n",
      "0\n",
      "6\n",
      "20\n",
      "34\n",
      "40\n",
      "52\n",
      "65\n",
      "[Batch 320/1456] elapsed 998.97 s\n",
      "73\n",
      "79\n",
      "88\n",
      "99\n",
      "107\n",
      "113\n",
      "[Batch 640/1456] elapsed 995.12 s\n",
      "124\n",
      "131\n",
      "141\n",
      "150\n",
      "155\n",
      "160\n",
      "168\n",
      "[Batch 960/1456] elapsed 992.96 s\n",
      "178\n",
      "185\n",
      "198\n",
      "206\n",
      "221\n",
      "231\n",
      "[Batch 1280/1456] elapsed 993.80 s\n",
      "241\n",
      "253\n",
      "258\n",
      "265\n",
      "19875 34428\n",
      "(34428, 650)\n",
      "\n",
      "\n",
      "start= 650 end= 700\n",
      "0\n",
      "10000\n",
      "20000\n",
      "34428\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,1000):\n",
    "    np.save('mask{}.npy'.format(i), mask)\n",
    "    np.save('np_recall_score{}.npy'.format(i), np_recall_score)\n",
    "\n",
    "    np_new_recall_score_idx = np.argsort(-np_recall_score, axis=1)\n",
    "    bert_rerank_train_val = np.take_along_axis(bm25_rank_val, np_new_recall_score_idx, axis=1)\n",
    "    val['ans0'] = bert_rerank_train_val[:,0]\n",
    "    val['ans1'] = bert_rerank_train_val[:,1]\n",
    "    val['ans2'] = bert_rerank_train_val[:,2]\n",
    "    def idx2name(idx):\n",
    "        return candid.iloc[idx].paper_id\n",
    "    val['ans0'] = val['ans0'].apply(idx2name)\n",
    "    val['ans1'] = val['ans1'].apply(idx2name)\n",
    "    val['ans2'] = val['ans2'].apply(idx2name)\n",
    "\n",
    "    val[['description_id','ans0','ans1','ans2']].to_csv('test_commit_{}_.csv'.format(i),\n",
    "                                                        index=False, header=None)\n",
    "    \n",
    "    start = i * k\n",
    "    end = (i+1) *k\n",
    "\n",
    "    print('start=',start,'end=',end)\n",
    "\n",
    "    gen_test_set(start,end,mask)\n",
    "    # cal score\n",
    "    np_recall_score_ext = np.hstack(get_score()).reshape([-1, k])\n",
    "    np_recall_score_ext = scatter_rows(mask,np_recall_score_ext,k=k)\n",
    "    \n",
    "    \n",
    "    if i>=9:\n",
    "        treshold = 2\n",
    "    mask_ = np.sum(np_recall_score_ext>treshold, axis=1)>0\n",
    "    mask = mask | mask_\n",
    "    print(sum(mask), len(mask))\n",
    "\n",
    "    np_recall_score = np.hstack([np_recall_score, np_recall_score_ext])\n",
    "    print(np_recall_score.shape)\n",
    "    \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

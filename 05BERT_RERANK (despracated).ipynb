{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from bert import data, model\n",
    "import time\n",
    "from mxnet.gluon.utils import split_and_load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "# change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "ctx = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3), mx.gpu(4), mx.gpu(5), mx.gpu(6), mx.gpu(7)]\n",
    "# ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',#book_corpus_wiki_en_uncased\n",
    "                                             dataset_name='biobert_v1.1_pubmed_cased', #biobert_v1.1_pubmed_cased\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier = model.classification.BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "bert_classifier.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "bert_classifier.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = mx.gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_classifier.load_parameters('bisai/epoch54399_acc0.90652.params')\n",
    "bert_classifier.load_parameters('bisai/512new/epoch121599.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_discard_samples = 1\n",
    "field_separator = nlp.data.Splitter('\\t')\n",
    "field_indices = [3, 4, 0]\n",
    "data_train_raw = nlp.data.TSVDataset(filename='val_bert_10.tsv', # change val_bert or test_bert_train_val\n",
    "                                 field_separator=field_separator,\n",
    "                                 num_discard_samples=num_discard_samples,\n",
    "                                 field_indices=field_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary used for tokenization = \n",
      "Vocab(size=28996, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
      "[PAD] token id = 0\n",
      "[CLS] token id = 101\n",
      "[SEP] token id = 102\n",
      "token ids = \n",
      "[  101 21902  6078  1105  8304 14758  1103   148 12240  1104   159  2271\n",
      "  2069  1116  1213 23645  6580  1133  1292  1127  8443  1107  3201 20562\n",
      "   117  1105  7377  1105  3605 14737  2522 10581  5660  1606  6145  2114\n",
      "  1104 11960  1116  1150  1125  2331  7372  1106  6860  1105  1127  8443\n",
      "  1121   170  1661  1107  4666   117  3066   164   164   115   115   108\n",
      "   108   115   115   166   166   119   102  1212  1103  7762  1206  8234\n",
      "  1105  2629 15156   119  1188  2526 26856  1103  8550  1104  8234  1105\n",
      "  2629 15156  1606   170  1326  1104  5136   119 11300 15022  1105  2629\n",
      " 15156  1132  5708  3393  1439  1103  4073  8057  5822  4746  8297   119\n",
      " 11300 15022  1110  3393  1107  2538  1104  1103  3154  1104   123 22496\n",
      "  6142  2629 15156  1110  3393  1107  2538  1104  1103  2629  1104  1141\n",
      "  9108  9507  1506   188 26649  1104   170  1248  7898   119 27007 15156\n",
      "  1169  1129  1675  1114  1185  8234   132  8234  1169  1129  1675  1114\n",
      "  1185  2629 15156   119  1247  1132 11106  1107  1134  1122  1110  1936\n",
      "  1106 15187  2629 15156  1133  1136  8234   117  1137  1106 15187  8234\n",
      "  1133  1136  2629 15156   119  1109 24443 23894  8826  1111 11621 10777\n",
      "  1104  2629 15156 11934  1105  8234 11934  1606 16404  8649  3584  1132\n",
      "  3402  1105 24681   119   138 27419  1110  1549  1104  1103 11106  1107\n",
      "  1134  8234  1105  2629 15156 21439   119   102     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "valid length = \n",
      "224\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=False)\n",
    "max_len = 512\n",
    "all_labels = [\"0\", \"UNKOWN\"]\n",
    "\n",
    "# whether to transform the data as sentence pairs.\n",
    "# for single sentence classification, set pair=False\n",
    "# for regression task, set class_labels=None\n",
    "# for inference without label available, set has_label=False\n",
    "pair = True\n",
    "transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                                class_labels=all_labels,\n",
    "                                                has_label=True,\n",
    "                                                pad=True,\n",
    "                                                pair=pair)\n",
    "\n",
    "\n",
    "data_train = data_train_raw.transform(transform)\n",
    "\n",
    "sample_id = 5\n",
    "print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))\n",
    "print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))\n",
    "print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))\n",
    "print('token ids = \\n%s'%data_train[sample_id][0])\n",
    "print('valid length = \\n%s'%data_train[sample_id][1])\n",
    "print('segment ids = \\n%s'%data_train[sample_id][2])\n",
    "print('label = \\n%s'%data_train[sample_id][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343120"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with only three epochs\n",
    "batch_size = 240\n",
    "bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_size=batch_size, num_workers=40,\n",
    "                                           pin_memory=True, prefetch=20000)\n",
    "\n",
    "log_interval = 160\n",
    "test_log_interval = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 320/1430] elapsed 455.13 s\n",
      "[Batch 640/1430] elapsed 459.74 s\n",
      "[Batch 960/1430] elapsed 461.03 s\n",
      "[Batch 1280/1430] elapsed 464.48 s\n"
     ]
    }
   ],
   "source": [
    "start_log_interval_time = time.time()\n",
    "new_recall_score = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(f):\n",
    "    # Load the data to the GPU\n",
    "#     token_ids = token_ids.as_in_context(ctx)\n",
    "#     valid_length = valid_length.as_in_context(ctx)\n",
    "#     segment_ids = segment_ids.as_in_context(ctx)\n",
    "\n",
    "#     label = label.as_in_context(ctx)\n",
    "\n",
    "    token_ids = split_and_load(token_ids, ctx, even_split=False)\n",
    "    valid_length = split_and_load(valid_length.astype('float32'), ctx, even_split=False)\n",
    "    segment_ids = split_and_load(segment_ids, ctx, even_split=False)\n",
    "    label = [bert_classifier(a, b, c)\n",
    "                  for a, b, c in zip(token_ids, segment_ids, valid_length)]\n",
    "    \n",
    "    label = [l[:,1].asnumpy() for l in label]\n",
    "    new_recall_score.extend(label)\n",
    "\n",
    "    if (batch_id + 1) % test_log_interval == 0:\n",
    "        print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "            batch_id + 1, len(bert_dataloader),\n",
    "            time.time() - start_log_interval_time))\n",
    "        start_log_interval_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bm25_rank_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a53b71724de4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbm25_rank_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# np_new_recall_score_idx.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bm25_rank_val' is not defined"
     ]
    }
   ],
   "source": [
    "bm25_rank_val.shape\n",
    "# np_new_recall_score_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_hdf('cleaned.h5', 'train')\n",
    "val = pd.read_hdf('cleaned.h5', 'val')\n",
    "candid = pd.read_hdf('cleaned.h5', 'candid')\n",
    "candid = candid.reset_index(drop=True)\n",
    "bm25_rank_train = np.load('train_recall.npy')\n",
    "bm25_rank_val = np.load('val_recall.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_new_recall_score = np.hstack(new_recall_score).reshape([-1, 10])\n",
    "np_new_recall_score_idx = np.argsort(-np_new_recall_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_rerank_train_val = np.take_along_axis(bm25_rank_train[60000:,:], np_new_recall_score_idx, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_new_recall_score_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_rerank_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_rerank_train_val = np.take_along_axis(bm25_rank_val, np_new_recall_score_idx, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['ans0'] = bert_rerank_train_val[:,0]\n",
    "val['ans1'] = bert_rerank_train_val[:,1]\n",
    "val['ans2'] = bert_rerank_train_val[:,2]\n",
    "def idx2name(idx):\n",
    "    return candid.iloc[idx].paper_id\n",
    "val['ans0'] = val['ans0'].apply(idx2name)\n",
    "val['ans1'] = val['ans1'].apply(idx2name)\n",
    "val['ans2'] = val['ans2'].apply(idx2name)\n",
    "\n",
    "val[['description_id','ans0','ans1','ans2']].to_csv('commit123.csv', index=False, header=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
